I copied heavily from Andrej Karpathy's "Let's build the GPT Tokenizer" to write this.
It's a tokenizer with a vocabulary size of 1256 trained on Mark Twain's lifetime works as retrieved from project gutenberg.
